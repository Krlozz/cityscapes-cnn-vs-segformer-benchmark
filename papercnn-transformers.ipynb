{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip -q install -U transformers accelerate\n",
    "#!pip install kagglehub\n",
    "#!pip install safetensors\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, json, time, random, platform\n",
    "from pathlib import Path\n",
    "from contextlib import nullcontext\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "from transformers import (\n",
    "    SegformerImageProcessor,\n",
    "    SegformerForSemanticSegmentation,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = Path(r\"C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\")\n",
    "OUTPUT_DIR  = PROJECT_DIR / \"outputs\"\n",
    "SPLITS_DIR  = OUTPUT_DIR / \"splits\"\n",
    "CKPT_DIR    = OUTPUT_DIR / \"checkpoints\"\n",
    "LOG_DIR     = OUTPUT_DIR / \"logs\"\n",
    "FIG_DIR     = OUTPUT_DIR / \"figures\"\n",
    "TABLE_DIR   = OUTPUT_DIR / \"tables\"\n",
    "STATE_PATH  = OUTPUT_DIR / \"run_state.json\"\n",
    "\n",
    "for d in [OUTPUT_DIR, SPLITS_DIR, CKPT_DIR, LOG_DIR, FIG_DIR, TABLE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "\n",
    "# Cityscapes training resolution\n",
    "RESIZE = (512, 1024)  # (H, W)\n",
    "CROP   = (512, 1024)\n",
    "\n",
    "NUM_CLASSES  = 19\n",
    "IGNORE_INDEX = 255\n",
    "\n",
    "# Baseline CNN training hyperparams\n",
    "CNN_EPOCHS    = 30\n",
    "CNN_BATCH     = 4\n",
    "CNN_LR        = 3e-4\n",
    "CNN_WD        = 1e-4\n",
    "CNN_PATIENCE  = 8\n",
    "\n",
    "# SegFormer training hyperparams\n",
    "SF_EPOCHS     = 30\n",
    "SF_BATCH      = 2\n",
    "SF_LR         = 6e-5\n",
    "SF_WD         = 1e-4\n",
    "SF_PATIENCE   = 8\n",
    "\n",
    "# Data-efficiency\n",
    "DATA_EFF_PCTS = [10, 25, 50, 100]\n",
    "\n",
    "# Efficiency measurement\n",
    "EFF_WARMUP_STEPS   = 10\n",
    "EFF_MEASURE_STEPS  = 50\n",
    "\n",
    "# Visualization\n",
    "N_VIS_SAMPLES = 3\n",
    "VIS_RANDOM_SEED = 999\n",
    "\n",
    "# Corruptions (Deliverable 3)\n",
    "CORRUPTIONS = [\n",
    "    (\"gaussian_blur\", 3),\n",
    "    (\"motion_blur\", 3),\n",
    "    (\"gaussian_noise\", 3),\n",
    "    (\"brightness\", 3),\n",
    "    (\"contrast\", 3),\n",
    "    (\"jpeg\", 3),\n",
    "]\n",
    "\n",
    "# HuggingFace SegFormer\n",
    "MODEL_ID = \"nvidia/segformer-b0-finetuned-cityscapes-512-1024\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disponibilidad de recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIN_MEMORY: True\n",
      "NUM_WORKERS: 0\n"
     ]
    }
   ],
   "source": [
    "PIN_MEMORY = (device.type == \"cuda\")\n",
    "# CPU_COUNT = os.cpu_count() or 4\n",
    "# NUM_WORKERS = min(8, max(2, CPU_COUNT // 2)) if platform.system().lower() == \"windows\" else 0\n",
    "\n",
    "IN_NOTEBOOK = (\"ipykernel\" in sys.modules) or (\"JPY_PARENT_PID\" in os.environ)\n",
    "if platform.system().lower() == \"windows\" and IN_NOTEBOOK:\n",
    "    # Prevent silent deadlock in Jupyter on Windows\n",
    "    NUM_WORKERS = 0\n",
    "else:\n",
    "    CPU_COUNT = os.cpu_count() or 4\n",
    "    NUM_WORKERS = min(8, max(2, CPU_COUNT // 2))\n",
    "\n",
    "print(\"PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count: 3\n",
      "  GPU 0: Quadro RTX 4000\n",
      "  GPU 1: Quadro P4000\n",
      "  GPU 2: Quadro P4000\n"
     ]
    }
   ],
   "source": [
    "gpu_count = torch.cuda.device_count()\n",
    "print(\"GPU count:\", gpu_count)\n",
    "for i in range(gpu_count):\n",
    "    print(f\"  GPU {i}:\", torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_DATAPARALLEL: True\n"
     ]
    }
   ],
   "source": [
    "FORCE_USE_ALL_GPUS = False\n",
    "USE_DATAPARALLEL = (device.type == \"cuda\" and gpu_count > 1)\n",
    "print(\"USE_DATAPARALLEL:\", USE_DATAPARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_dp_device_ids(min_ratio=0.75):\n",
    "    if gpu_count < 2:\n",
    "        return None\n",
    "    props = [torch.cuda.get_device_properties(i) for i in range(gpu_count)]\n",
    "    mem0 = props[0].total_memory\n",
    "    sm0  = props[0].multi_processor_count\n",
    "    bad = []\n",
    "    for i in range(1, gpu_count):\n",
    "        mem_ratio = props[i].total_memory / mem0\n",
    "        sm_ratio  = props[i].multi_processor_count / sm0\n",
    "        ratio = min(mem_ratio, sm_ratio)\n",
    "        if ratio < min_ratio:\n",
    "            bad.append((i, mem_ratio, sm_ratio))\n",
    "    if bad:\n",
    "        print(\"DataParallel imbalance detectado. Usaré SOLO GPU0.\")\n",
    "        for (i, mr, sr) in bad:\n",
    "            print(f\" - GPU{i}: mem_ratio={mr:.2f}, sm_ratio={sr:.2f} (< {min_ratio})\")\n",
    "        return [0]\n",
    "    return list(range(gpu_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_DEVICE_IDS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel imbalance detectado. Usaré SOLO GPU0.\n",
      " - GPU1: mem_ratio=1.00, sm_ratio=0.39 (< 0.75)\n",
      " - GPU2: mem_ratio=1.00, sm_ratio=0.39 (< 0.75)\n"
     ]
    }
   ],
   "source": [
    "if USE_DATAPARALLEL:\n",
    "    DP_DEVICE_IDS = list(range(gpu_count)) if FORCE_USE_ALL_GPUS else choose_dp_device_ids(0.75)\n",
    "    if DP_DEVICE_IDS == [0]:\n",
    "        USE_DATAPARALLEL = False\n",
    "        DP_DEVICE_IDS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_DATAPARALLEL: False\n",
      "DP_DEVICE_IDS   : None\n"
     ]
    }
   ],
   "source": [
    "print(\"USE_DATAPARALLEL:\", USE_DATAPARALLEL)\n",
    "print(\"DP_DEVICE_IDS   :\", DP_DEVICE_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocast_ctx():\n",
    "    if device.type != \"cuda\":\n",
    "        return nullcontext()\n",
    "    return torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler():\n",
    "    if device.type != \"cuda\":\n",
    "        return None\n",
    "    return torch.amp.GradScaler(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state():\n",
    "    if STATE_PATH.exists():\n",
    "        return json.loads(STATE_PATH.read_text(encoding=\"utf-8\"))\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(st: dict):\n",
    "    STATE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    STATE_PATH.write_text(json.dumps(st, indent=2), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_done(key: str, value=True):\n",
    "    st = load_state()\n",
    "    st[key] = value\n",
    "    save_state(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_done(key: str):\n",
    "    return bool(load_state().get(key, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_pct_done(model_key: str, pct: int, value=True):\n",
    "    st = load_state()\n",
    "    if model_key not in st:\n",
    "        st[model_key] = {}\n",
    "    st[model_key][str(pct)] = value\n",
    "    save_state(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pct_done(model_key: str, pct: int):\n",
    "    st = load_state()\n",
    "    return bool(st.get(model_key, {}).get(str(pct), False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descarga de Dataset Cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cityscapes_with_kagglehub():\n",
    "    path = kagglehub.dataset_download(\"electraawais/cityscape-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    return Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\ALLAN TURING\\.cache\\kagglehub\\datasets\\electraawais\\cityscape-dataset\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = download_cityscapes_with_kagglehub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_cityscapes_paths(root: Path):\n",
    "    known_img = root / \"Cityscape Dataset\" / \"leftImg8bit\"\n",
    "    known_gt  = root / \"Fine Annotations\" / \"gtFine\"\n",
    "    if known_img.is_dir() and known_gt.is_dir():\n",
    "        return known_img, known_gt\n",
    "\n",
    "    left_candidates = []\n",
    "    gt_candidates   = []\n",
    "    for p in root.rglob(\"leftImg8bit\"):\n",
    "        if (p / \"train\").is_dir() and (p / \"val\").is_dir():\n",
    "            left_candidates.append(p)\n",
    "    for p in root.rglob(\"gtFine\"):\n",
    "        if (p / \"train\").is_dir() and (p / \"val\").is_dir():\n",
    "            gt_candidates.append(p)\n",
    "\n",
    "    if not left_candidates or not gt_candidates:\n",
    "        raise FileNotFoundError(\"No encontré leftImg8bit/gtFine con train/val en el dataset descargado.\")\n",
    "    return left_candidates[0], gt_candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ROOT, GT_ROOT = resolve_cityscapes_paths(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_ROOT: C:\\Users\\ALLAN TURING\\.cache\\kagglehub\\datasets\\electraawais\\cityscape-dataset\\versions\\2\\Cityscape Dataset\\leftImg8bit\n",
      "GT_ROOT : C:\\Users\\ALLAN TURING\\.cache\\kagglehub\\datasets\\electraawais\\cityscape-dataset\\versions\\2\\Fine Annotations\\gtFine\n"
     ]
    }
   ],
   "source": [
    "print(\"IMG_ROOT:\", IMG_ROOT)\n",
    "print(\"GT_ROOT :\", GT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    assert (IMG_ROOT / sp).is_dir(), f\"Falta {sp} en leftImg8bit\"\n",
    "    assert (GT_ROOT / sp).is_dir(), f\"Falta {sp} en gtFine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de pares (imagen - máscara) por split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_pairs(split: str):\n",
    "    img_glob = str(IMG_ROOT / split / \"*\" / \"*_leftImg8bit.png\")\n",
    "    imgs = sorted(glob.glob(img_glob))\n",
    "    pairs, missing = [], 0\n",
    "    for ip in imgs:\n",
    "        ip = Path(ip)\n",
    "        city = ip.parent.name\n",
    "        stem = ip.name.replace(\"_leftImg8bit.png\", \"\")\n",
    "        mp = GT_ROOT / split / city / f\"{stem}_gtFine_labelIds.png\"\n",
    "        if mp.exists():\n",
    "            pairs.append((str(ip), str(mp)))\n",
    "        else:\n",
    "            missing += 1\n",
    "    print(f\"[{split}] imgs={len(imgs)} paired={len(pairs)} missing_masks={missing}\")\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] imgs=2975 paired=2975 missing_masks=0\n",
      "[val] imgs=500 paired=500 missing_masks=0\n",
      "[test] imgs=1525 paired=1525 missing_masks=0\n"
     ]
    }
   ],
   "source": [
    "train_pairs_full = collect_pairs(\"train\")\n",
    "val_pairs  = collect_pairs(\"val\")\n",
    "test_pairs = collect_pairs(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_pairs_full) > 0 and len(val_pairs) > 0, \"No hay pares suficientes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_split(pairs, filename):\n",
    "    outp = SPLITS_DIR / filename\n",
    "    with open(outp, \"w\", encoding=\"utf-8\") as f:\n",
    "        for img, mask in pairs:\n",
    "            f.write(f\"{img}\\t{mask}\\n\")\n",
    "    return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits guardados:\n",
      " - C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\splits\\train.txt\n",
      " - C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\splits\\val.txt\n",
      " - C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\splits\\test.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Splits guardados:\")\n",
    "print(\" -\", write_split(train_pairs_full, \"train.txt\"))\n",
    "print(\" -\", write_split(val_pairs, \"val.txt\"))\n",
    "print(\" -\", write_split(test_pairs, \"test.txt\"))\n",
    "mark_done(\"splits_written\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainlds 19 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_TO_TRAINID = np.ones(256, dtype=np.uint8) * IGNORE_INDEX\n",
    "mapping = {\n",
    "    7: 0,   # road\n",
    "    8: 1,   # sidewalk\n",
    "    11: 2,  # building\n",
    "    12: 3,  # wall\n",
    "    13: 4,  # fence\n",
    "    17: 5,  # pole\n",
    "    19: 6,  # traffic light\n",
    "    20: 7,  # traffic sign\n",
    "    21: 8,  # vegetation\n",
    "    22: 9,  # terrain\n",
    "    23: 10, # sky\n",
    "    24: 11, # person\n",
    "    25: 12, # rider\n",
    "    26: 13, # car\n",
    "    27: 14, # truck\n",
    "    28: 15, # bus\n",
    "    31: 16, # train\n",
    "    32: 17, # motorcycle\n",
    "    33: 18  # bicycle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in mapping.items():\n",
    "    ID_TO_TRAINID[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0:\"road\",1:\"sidewalk\",2:\"building\",3:\"wall\",4:\"fence\",5:\"pole\",\n",
    "    6:\"traffic_light\",7:\"traffic_sign\",8:\"vegetation\",9:\"terrain\",\n",
    "    10:\"sky\",11:\"person\",12:\"rider\",13:\"car\",14:\"truck\",15:\"bus\",\n",
    "    16:\"train\",17:\"motorcycle\",18:\"bicycle\"\n",
    "}\n",
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED = {\"road\":0, \"person\":11, \"car\":13, \"building\":2, \"vegetation\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(3,1,1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_min_size(img_np, mask_np, min_h, min_w):\n",
    "    h, w = img_np.shape[0], img_np.shape[1]\n",
    "    pad_h = max(0, min_h - h)\n",
    "    pad_w = max(0, min_w - w)\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        img_np = np.pad(img_np, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "        mask_np = np.pad(mask_np, ((0, pad_h), (0, pad_w)), mode=\"constant\", constant_values=IGNORE_INDEX)\n",
    "    return img_np, mask_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img_np, mask_np, crop_h, crop_w):\n",
    "    h, w = img_np.shape[0], img_np.shape[1]\n",
    "    y0 = random.randint(0, h - crop_h)\n",
    "    x0 = random.randint(0, w - crop_w)\n",
    "    return img_np[y0:y0+crop_h, x0:x0+crop_w, :], mask_np[y0:y0+crop_h, x0:x0+crop_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_jitter_np(img_np, brightness=0.2, contrast=0.2, saturation=0.2):\n",
    "    b = 1.0 + random.uniform(-brightness, brightness)\n",
    "    c = 1.0 + random.uniform(-contrast, contrast)\n",
    "    s = 1.0 + random.uniform(-saturation, saturation)\n",
    "    img = np.clip(img_np * b, 0, 1)\n",
    "    mean = img.mean(axis=(0,1), keepdims=True)\n",
    "    img = np.clip((img - mean) * c + mean, 0, 1)\n",
    "    gray = (img[...,0]*0.299 + img[...,1]*0.587 + img[...,2]*0.114)[...,None]\n",
    "    img = np.clip((img - gray) * s + gray, 0, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_color_jitter(img_pil, b=0.2, c=0.2, s=0.2):\n",
    "    img_pil = ImageEnhance.Brightness(img_pil).enhance(1.0 + random.uniform(-b, b))\n",
    "    img_pil = ImageEnhance.Contrast(img_pil).enhance(1.0 + random.uniform(-c, c))\n",
    "    img_pil = ImageEnhance.Color(img_pil).enhance(1.0 + random.uniform(-s, s))\n",
    "    return img_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_motion_blur(img_pil: Image.Image, k: int = 9):\n",
    "    k = int(k)\n",
    "    if k < 3:\n",
    "        k = 3\n",
    "    if k % 2 == 0:\n",
    "        k += 1\n",
    "    k = min(k, 15)\n",
    "    \n",
    "    try:\n",
    "        import cv2\n",
    "        arr = np.array(img_pil.convert(\"RGB\"), dtype=np.uint8)\n",
    "        kernel = np.zeros((k, k), dtype=np.float32)\n",
    "        kernel[k // 2, :] = 1.0 / k\n",
    "        out = cv2.filter2D(arr, ddepth=-1, kernel=kernel)\n",
    "        return Image.fromarray(out, mode=\"RGB\")\n",
    "    except Exception:\n",
    "        radius = max(1, k // 4)\n",
    "        return img_pil.filter(ImageFilter.GaussianBlur(radius=float(radius)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_noise(img_np_uint8, sigma: float = 15.0):\n",
    "    noise = np.random.normal(0, float(sigma), img_np_uint8.shape).astype(np.float32)\n",
    "    out = np.clip(img_np_uint8.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_jpeg_compression(img_pil: Image.Image, quality: int = 30):\n",
    "    buf = BytesIO()\n",
    "    img_pil.convert(\"RGB\").save(buf, format=\"JPEG\", quality=int(quality))\n",
    "    buf.seek(0)\n",
    "    out = Image.open(buf).convert(\"RGB\").copy()  # copy() evita problemas en workers\n",
    "    buf.close()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_corruption(img_pil: Image.Image, corruption: str, severity: int = 1):\n",
    "    severity = int(np.clip(int(severity), 1, 5))\n",
    "\n",
    "    try:\n",
    "        if corruption == \"gaussian_blur\":\n",
    "            radius = [0.5, 1.0, 1.5, 2.0, 3.0][severity-1]\n",
    "            return img_pil.filter(ImageFilter.GaussianBlur(radius=float(radius)))\n",
    "\n",
    "        if corruption == \"motion_blur\":\n",
    "            k = [5, 7, 9, 11, 15][severity-1]\n",
    "            return apply_motion_blur(img_pil, k=k)\n",
    "\n",
    "        if corruption == \"gaussian_noise\":\n",
    "            sigma = [5, 10, 15, 25, 35][severity-1]\n",
    "            arr = np.array(img_pil.convert(\"RGB\"), dtype=np.uint8)\n",
    "            arr = apply_gaussian_noise(arr, sigma=sigma)\n",
    "            return Image.fromarray(arr, mode=\"RGB\")\n",
    "\n",
    "        if corruption == \"brightness\":\n",
    "            factor = [0.7, 0.6, 0.5, 0.4, 0.3][severity-1]\n",
    "            return ImageEnhance.Brightness(img_pil).enhance(float(factor))\n",
    "\n",
    "        if corruption == \"contrast\":\n",
    "            factor = [0.8, 0.7, 0.6, 0.5, 0.4][severity-1]\n",
    "            return ImageEnhance.Contrast(img_pil).enhance(float(factor))\n",
    "\n",
    "        if corruption == \"jpeg\":\n",
    "            quality = [60, 45, 30, 20, 10][severity-1]\n",
    "            return apply_jpeg_compression(img_pil, quality=int(quality))\n",
    "\n",
    "        return img_pil\n",
    "\n",
    "    except Exception as e:\n",
    "        if not hasattr(apply_corruption, \"_warned\"):\n",
    "            print(f\"[WARN] Corruption '{corruption}' failed (severity={severity}): {e}. Using clean image.\")\n",
    "            apply_corruption._warned = True\n",
    "        return img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_blur\n",
      "motion_blur\n",
      "gaussian_noise\n",
      "brightness\n",
      "contrast\n",
      "jpeg\n",
      "Corruptions OK\n"
     ]
    }
   ],
   "source": [
    "### Test de corruptions ok\n",
    "img = Image.new(\"RGB\", (512, 1024), color=(128,128,128))\n",
    "for corr in [\"gaussian_blur\",\"motion_blur\",\"gaussian_noise\",\"brightness\",\"contrast\",\"jpeg\"]:\n",
    "    for s in range(1,6):\n",
    "        _ = apply_corruption(img, corr, s)\n",
    "    print(corr)\n",
    "print(\"Corruptions OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets (CNN y SegFormer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDatasetCNN(Dataset):\n",
    "    def __init__(self, pairs, resize=RESIZE, crop=CROP, train=False, corruption=None, severity=1):\n",
    "        self.pairs = pairs\n",
    "        self.resize = resize\n",
    "        self.crop = crop\n",
    "        self.train = train\n",
    "        self.corruption = corruption\n",
    "        self.severity = severity\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_path, mask_path = self.pairs[i]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        img  = img.resize((self.resize[1], self.resize[0]), resample=Image.BILINEAR)\n",
    "        mask = mask.resize((self.resize[1], self.resize[0]), resample=Image.NEAREST)\n",
    "\n",
    "        if (not self.train) and (self.corruption is not None):\n",
    "            img = apply_corruption(img, self.corruption, self.severity)\n",
    "\n",
    "        img_np  = np.array(img, dtype=np.float32) / 255.0\n",
    "        mask_np = ID_TO_TRAINID[np.array(mask, dtype=np.uint8)].astype(np.int64)\n",
    "\n",
    "        if self.train:\n",
    "            if random.random() < 0.5:\n",
    "                img_np  = np.ascontiguousarray(img_np[:, ::-1, :])\n",
    "                mask_np = np.ascontiguousarray(mask_np[:, ::-1])\n",
    "            img_np = color_jitter_np(img_np, 0.2, 0.2, 0.2)\n",
    "\n",
    "            ch, cw = self.crop\n",
    "            img_np, mask_np = pad_to_min_size(img_np, mask_np, ch, cw)\n",
    "            img_np, mask_np = random_crop(img_np, mask_np, ch, cw)\n",
    "\n",
    "        img_t = torch.from_numpy(img_np).permute(2,0,1).contiguous()\n",
    "        img_t = (img_t - IMAGENET_MEAN) / IMAGENET_STD\n",
    "        mask_t = torch.from_numpy(mask_np.astype(np.int64))\n",
    "        return img_t, mask_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALLAN TURING\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_base.py:417: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n"
     ]
    }
   ],
   "source": [
    "processor = SegformerImageProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    do_rescale=True,\n",
    "    do_normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDatasetSF(Dataset):\n",
    "    def __init__(self, pairs, resize=RESIZE, crop=CROP, train=False, corruption=None, severity=1):\n",
    "        self.pairs = pairs\n",
    "        self.resize = resize\n",
    "        self.crop = crop\n",
    "        self.train = train\n",
    "        self.corruption = corruption\n",
    "        self.severity = severity\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_path, mask_path = self.pairs[i]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        img  = img.resize((self.resize[1], self.resize[0]), resample=Image.BILINEAR)\n",
    "        mask = mask.resize((self.resize[1], self.resize[0]), resample=Image.NEAREST)\n",
    "\n",
    "        if (not self.train) and (self.corruption is not None):\n",
    "            img = apply_corruption(img, self.corruption, self.severity)\n",
    "\n",
    "        mask_np = ID_TO_TRAINID[np.array(mask, dtype=np.uint8)].astype(np.int64)\n",
    "\n",
    "        if self.train:\n",
    "            if random.random() < 0.5:\n",
    "                img_np  = np.array(img, dtype=np.uint8)\n",
    "                img_np  = np.ascontiguousarray(img_np[:, ::-1, :])\n",
    "                mask_np = np.ascontiguousarray(mask_np[:, ::-1])\n",
    "                img = Image.fromarray(img_np)\n",
    "            img = pil_color_jitter(img, b=0.2, c=0.2, s=0.2)\n",
    "\n",
    "            img_np = np.array(img, dtype=np.uint8)\n",
    "            ch, cw = self.crop\n",
    "            img_np, mask_np = pad_to_min_size(img_np, mask_np, ch, cw)\n",
    "            img_np, mask_np = random_crop(img_np, mask_np, ch, cw)\n",
    "            img = Image.fromarray(img_np)\n",
    "\n",
    "        enc = processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = enc[\"pixel_values\"].squeeze(0)\n",
    "        labels = torch.from_numpy(mask_np.astype(np.int64))\n",
    "        return pixel_values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def update_confusion(conf, preds, labels, num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX):\n",
    "    valid = labels != ignore_index\n",
    "    gt = labels[valid].view(-1)\n",
    "    pd = preds[valid].view(-1)\n",
    "    k = (gt * num_classes + pd).to(torch.int64)\n",
    "    conf += torch.bincount(k, minlength=num_classes*num_classes).reshape(num_classes, num_classes)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_from_conf(conf):\n",
    "    tp = conf.diag().float()\n",
    "    fp = conf.sum(0).float() - tp\n",
    "    fn = conf.sum(1).float() - tp\n",
    "    iou = tp / (tp + fp + fn + 1e-6)\n",
    "    miou = iou.mean().item()\n",
    "    return miou, iou.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_acc_from_conf(conf):\n",
    "    correct = conf.diag().sum().item()\n",
    "    total = conf.sum().item()\n",
    "    return correct / (total + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_class_iou(iou_arr):\n",
    "    return {k: float(iou_arr[idx]) for k, idx in SELECTED.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(num_classes=NUM_CLASSES):\n",
    "    try:\n",
    "        backbone_w = ResNet50_Weights.DEFAULT\n",
    "    except Exception:\n",
    "        backbone_w = None\n",
    "\n",
    "    try:\n",
    "        model = deeplabv3_resnet50(weights=None, weights_backbone=backbone_w, num_classes=num_classes)\n",
    "    except Exception:\n",
    "        model = deeplabv3_resnet50(weights=None, weights_backbone=None, num_classes=num_classes)\n",
    "\n",
    "    if USE_DATAPARALLEL and DP_DEVICE_IDS is not None:\n",
    "        model = nn.DataParallel(model, device_ids=DP_DEVICE_IDS)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        out = self.base(pixel_values=pixel_values)\n",
    "        return {\"logits\": out.logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_segformer_model(model_id=MODEL_ID):\n",
    "    base = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    model = SegFormerWrapper(base)\n",
    "    if USE_DATAPARALLEL and DP_DEVICE_IDS is not None:\n",
    "        model = nn.DataParallel(model, device_ids=DP_DEVICE_IDS)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pth_weights(model, path: Path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    torch.save(state, str(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pth_weights(model, path: Path, strict=True):\n",
    "    path = Path(path)\n",
    "    state = torch.load(str(path), map_location=\"cpu\", weights_only=True)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(state, strict=strict)\n",
    "    else:\n",
    "        model.load_state_dict(state, strict=strict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_cnn(model, loader):\n",
    "    model.eval()\n",
    "    conf = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64, device=device)\n",
    "    for imgs, masks in loader:\n",
    "        imgs  = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        out = model(imgs)[\"out\"]\n",
    "        preds = out.argmax(1)\n",
    "        conf = update_confusion(conf, preds, masks)\n",
    "    miou, iou_arr = metrics_from_conf(conf)\n",
    "    acc = pixel_acc_from_conf(conf)\n",
    "    return miou, acc, selected_class_iou(iou_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_sf(model_sf, loader):\n",
    "    model_sf.eval()\n",
    "    conf = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64, device=device)\n",
    "    for px, labels in loader:\n",
    "        px     = px.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        logits = model_sf(pixel_values=px)[\"logits\"]\n",
    "        logits = F.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        preds  = logits.argmax(1)\n",
    "        conf = update_confusion(conf, preds, labels)\n",
    "    miou, iou_arr = metrics_from_conf(conf)\n",
    "    acc = pixel_acc_from_conf(conf)\n",
    "    return miou, acc, selected_class_iou(iou_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_cnn(model, loader, criterion, optimizer, scaler, epoch, epochs):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"CNN Epoch {epoch}/{epochs}\", leave=True)\n",
    "    t0 = time.time()\n",
    "    for step, (imgs, masks) in enumerate(pbar, start=1):\n",
    "        imgs  = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast_ctx():\n",
    "            out = model(imgs)[\"out\"]\n",
    "            loss = criterion(out, masks)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running += float(loss.item())\n",
    "        avg_loss = running / step\n",
    "        ms_step = ((time.time() - t0) / step) * 1000\n",
    "        pbar.set_postfix(loss=f\"{avg_loss:.4f}\", ms_step=f\"{ms_step:.0f}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    return running / max(len(loader), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_sf(model_sf, loader, optimizer, scheduler, scaler, epoch, epochs):\n",
    "    model_sf.train()\n",
    "    running = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"SF Epoch {epoch}/{epochs}\", leave=True)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step, (px, labels) in enumerate(pbar, start=1):\n",
    "        px = px.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast_ctx():\n",
    "            out = model_sf(pixel_values=px)\n",
    "            logits = out[\"logits\"]\n",
    "            logits = F.interpolate(\n",
    "                logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            loss = F.cross_entropy(logits, labels, ignore_index=IGNORE_INDEX)\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        running += float(loss.item())\n",
    "        avg_loss = running / step\n",
    "        ms_step = ((time.time() - t0) / step) * 1000\n",
    "        pbar.set_postfix(loss=f\"{avg_loss:.4f}\", ms_step=f\"{ms_step:.0f}\",\n",
    "                         lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    return running / max(len(loader), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_subset(pairs, pct, seed=SEED):\n",
    "    pct = int(pct)\n",
    "    assert pct in [10, 25, 50, 100]\n",
    "    if pct == 100:\n",
    "        return pairs\n",
    "    n = len(pairs)\n",
    "    m = max(1, int(n * (pct / 100.0)))\n",
    "    rng = np.random.RandomState(seed + pct)\n",
    "    idx = rng.permutation(n)[:m]\n",
    "    return [pairs[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader_cnn(pairs, train: bool, corruption=None, severity=1, batch_size=None):\n",
    "    bs = CNN_BATCH if batch_size is None else batch_size\n",
    "    ds = CityscapesDatasetCNN(pairs, resize=RESIZE, crop=CROP, train=train, corruption=corruption, severity=severity)\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=train, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader_sf(pairs, train: bool, corruption=None, severity=1, batch_size=None):\n",
    "    bs = SF_BATCH if batch_size is None else batch_size\n",
    "    ds = CityscapesDatasetSF(pairs, resize=RESIZE, crop=CROP, train=train, corruption=corruption, severity=severity)\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=train, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_row_csv(path: Path, key_cols, row_dict):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=list(row_dict.keys()))\n",
    "    for c in row_dict.keys():\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    if len(df) == 0:\n",
    "        df = pd.DataFrame([row_dict])\n",
    "    else:\n",
    "        mask = np.ones(len(df), dtype=bool)\n",
    "        for kc in key_cols:\n",
    "            mask &= (df[kc].astype(str) == str(row_dict[kc]))\n",
    "        if mask.any():\n",
    "            df.loc[mask, list(row_dict.keys())] = list(row_dict.values())\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame([row_dict])], ignore_index=True)\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_pct(pct: int):\n",
    "    ckpt_path = CKPT_DIR / f\"best_cnn_pct{pct}.pth\"\n",
    "    hist_path = LOG_DIR  / f\"history_cnn_pct{pct}.csv\"\n",
    "\n",
    "    if ckpt_path.exists() and is_pct_done(\"cnn_done\", pct):\n",
    "        print(f\"[SKIP] CNN pct={pct} ya entrenado. CKPT: {ckpt_path}\")\n",
    "        model = build_cnn_model()\n",
    "        load_pth_weights(model, ckpt_path)\n",
    "        return model, ckpt_path, hist_path\n",
    "\n",
    "    train_pairs = make_train_subset(train_pairs_full, pct, seed=SEED)\n",
    "    train_loader = make_loader_cnn(train_pairs, train=True)\n",
    "    val_loader   = make_loader_cnn(val_pairs, train=False)\n",
    "\n",
    "    # Smoke test (detect hang early)\n",
    "    print(\"[DEBUG] CNN loader smoke test...\")\n",
    "    b = next(iter(train_loader))\n",
    "    print(\"[DEBUG] CNN batch OK:\", b[0].shape, b[1].shape)\n",
    "\n",
    "    model = build_cnn_model()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CNN_LR, weight_decay=CNN_WD)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CNN_EPOCHS)\n",
    "    scaler = get_scaler()\n",
    "\n",
    "    best_miou = -1.0\n",
    "    bad = 0\n",
    "    rows = []\n",
    "\n",
    "    for e in range(1, CNN_EPOCHS + 1):\n",
    "        loss = train_one_epoch_cnn(model, train_loader, criterion, optimizer, scaler, e, CNN_EPOCHS)\n",
    "        miou, acc, cls = evaluate_cnn(model, val_loader)\n",
    "\n",
    "        improved = False\n",
    "        if miou > best_miou:\n",
    "            best_miou = miou\n",
    "            bad = 0\n",
    "            save_pth_weights(model, ckpt_path)\n",
    "            improved = True\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[CNN pct={pct}] Epoch {e}/{CNN_EPOCHS} | loss={loss:.4f} | val mIoU={miou:.4f} | acc={acc:.4f} {'(best)' if improved else ''}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"pct\": pct, \"epoch\": e,\n",
    "            \"train_loss\": loss, \"val_miou\": miou, \"val_acc\": acc,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "            \"iou_road\": cls[\"road\"], \"iou_person\": cls[\"person\"], \"iou_car\": cls[\"car\"],\n",
    "            \"iou_building\": cls[\"building\"], \"iou_vegetation\": cls[\"vegetation\"]\n",
    "        })\n",
    "        pd.DataFrame(rows).to_csv(hist_path, index=False)\n",
    "\n",
    "        if bad >= CNN_PATIENCE:\n",
    "            print(f\"[CNN pct={pct}] Early stopping.\")\n",
    "            break\n",
    "\n",
    "    mark_pct_done(\"cnn_done\", pct, True)\n",
    "    load_pth_weights(model, ckpt_path)\n",
    "    return model, ckpt_path, hist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_train_cnn_all_pcts():\n",
    "    out_summary = TABLE_DIR / \"cnn_data_efficiency_summary.csv\"\n",
    "\n",
    "    if is_done(\"cnn_data_eff_done\"):\n",
    "        print(\"[SKIP] CNN data-efficiency ya completado, no re-evaluo pcts.\")\n",
    "        return load_best_cnn_100()\n",
    "        \n",
    "    best_cnn_100 = None\n",
    "\n",
    "    for pct in DATA_EFF_PCTS:\n",
    "        print(f\"CNN DATA EFFICIENCY: pct={pct} | train_samples={len(make_train_subset(train_pairs_full, pct))}\")\n",
    "        cnn_model, cnn_ckpt, cnn_hist = train_cnn_pct(pct)\n",
    "\n",
    "        # Store best val (evaluate now)\n",
    "        val_loader = make_loader_cnn(val_pairs, train=False)\n",
    "        best_val_miou, best_val_acc, _ = evaluate_cnn(cnn_model, val_loader)\n",
    "\n",
    "        upsert_row_csv(out_summary, key_cols=[\"pct\"], row_dict={\n",
    "            \"pct\": pct,\n",
    "            \"best_val_miou\": best_val_miou,\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"cnn_ckpt\": str(cnn_ckpt),\n",
    "            \"cnn_hist\": str(cnn_hist),\n",
    "        })\n",
    "        print(\"Saved/Updated:\", out_summary)\n",
    "\n",
    "        if pct == 100:\n",
    "            best_cnn_100 = cnn_model\n",
    "\n",
    "    mark_done(\"cnn_data_eff_done\", True)\n",
    "    return best_cnn_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sf_pct(pct: int):\n",
    "    ckpt_path = CKPT_DIR / f\"best_sf_pct{pct}.pth\"\n",
    "    hist_path = LOG_DIR  / f\"history_sf_pct{pct}.csv\"\n",
    "\n",
    "    if ckpt_path.exists() and is_pct_done(\"sf_done\", pct):\n",
    "        print(f\"[SKIP] SF pct={pct} ya entrenado. CKPT: {ckpt_path}\")\n",
    "        model = build_segformer_model()\n",
    "        load_pth_weights(model, ckpt_path, strict=False)\n",
    "        return model, ckpt_path, hist_path\n",
    "\n",
    "    train_pairs = make_train_subset(train_pairs_full, pct, seed=SEED)\n",
    "    train_loader = make_loader_sf(train_pairs, train=True)\n",
    "    val_loader   = make_loader_sf(val_pairs, train=False)\n",
    "\n",
    "    # Smoke test (detect hang early)\n",
    "    print(\"[DEBUG] SF loader smoke test...\")\n",
    "    b = next(iter(train_loader))\n",
    "    print(\"[DEBUG] SF batch OK:\", b[0].shape, b[1].shape)\n",
    "\n",
    "    model = build_segformer_model()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=SF_LR, weight_decay=SF_WD)\n",
    "    total_steps  = SF_EPOCHS * len(train_loader)\n",
    "    warmup_steps = int(0.10 * total_steps)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    scaler = get_scaler()\n",
    "\n",
    "    best_miou = -1.0\n",
    "    bad = 0\n",
    "    rows = []\n",
    "\n",
    "    for e in range(1, SF_EPOCHS + 1):\n",
    "        loss = train_one_epoch_sf(model, train_loader, optimizer, scheduler, scaler, e, SF_EPOCHS)\n",
    "        miou, acc, cls = evaluate_sf(model, val_loader)\n",
    "\n",
    "        improved = False\n",
    "        if miou > best_miou:\n",
    "            best_miou = miou\n",
    "            bad = 0\n",
    "            save_pth_weights(model, ckpt_path)\n",
    "            improved = True\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        print(f\"[SF pct={pct}] Epoch {e}/{SF_EPOCHS} | loss={loss:.4f} | val mIoU={miou:.4f} | acc={acc:.4f} {'(best)' if improved else ''}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"pct\": pct, \"epoch\": e,\n",
    "            \"train_loss\": loss, \"val_miou\": miou, \"val_acc\": acc,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "            \"iou_road\": cls[\"road\"], \"iou_person\": cls[\"person\"], \"iou_car\": cls[\"car\"],\n",
    "            \"iou_building\": cls[\"building\"], \"iou_vegetation\": cls[\"vegetation\"]\n",
    "        })\n",
    "        pd.DataFrame(rows).to_csv(hist_path, index=False)\n",
    "\n",
    "        if bad >= SF_PATIENCE:\n",
    "            print(f\"[SF pct={pct}] Early stopping.\")\n",
    "            break\n",
    "\n",
    "    mark_pct_done(\"sf_done\", pct, True)\n",
    "    load_pth_weights(model, ckpt_path, strict=False)\n",
    "    return model, ckpt_path, hist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_train_sf_all_pcts():\n",
    "    out_summary = TABLE_DIR / \"sf_data_efficiency_summary.csv\"\n",
    "    best_sf_100 = None\n",
    "\n",
    "    for pct in DATA_EFF_PCTS:\n",
    "        print(f\"SF DATA EFFICIENCY: pct={pct} | train_samples={len(make_train_subset(train_pairs_full, pct))}\")\n",
    "        sf_model, sf_ckpt, sf_hist = train_sf_pct(pct)\n",
    "\n",
    "        # Store best val (evaluate now)\n",
    "        val_loader = make_loader_sf(val_pairs, train=False)\n",
    "        best_val_miou, best_val_acc, _ = evaluate_sf(sf_model, val_loader)\n",
    "\n",
    "        upsert_row_csv(out_summary, key_cols=[\"pct\"], row_dict={\n",
    "            \"pct\": pct,\n",
    "            \"best_val_miou\": best_val_miou,\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"sf_ckpt\": str(sf_ckpt),\n",
    "            \"sf_hist\": str(sf_hist),\n",
    "        })\n",
    "        print(\"Saved/Updated:\", out_summary)\n",
    "\n",
    "        if pct == 100:\n",
    "            best_sf_100 = sf_model\n",
    "\n",
    "    mark_done(\"sf_data_eff_done\", True)\n",
    "    return best_sf_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_plot_curve_cnn():\n",
    "    csv_path = TABLE_DIR / \"cnn_data_efficiency_summary.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(\"[WARN] No existe cnn_data_efficiency_summary.csv. Entrena CNN primero.\")\n",
    "        return\n",
    "    df = pd.read_csv(csv_path).sort_values(\"pct\")\n",
    "    fig_path = FIG_DIR / \"data_efficiency_curve_cnn.png\"\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df[\"pct\"], df[\"best_val_miou\"], marker=\"o\", label=\"CNN (DeepLabV3)\")\n",
    "    plt.xlabel(\"% training data\"); plt.ylabel(\"Best Val mIoU\")\n",
    "    plt.title(\"Data efficiency (CNN): mIoU vs % training data\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=200); plt.close()\n",
    "    print(\"Saved:\", fig_path)\n",
    "    mark_done(\"curve_cnn_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_plot_curve_sf():\n",
    "    csv_path = TABLE_DIR / \"sf_data_efficiency_summary.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(\"[WARN] No existe sf_data_efficiency_summary.csv. Entrena SF primero.\")\n",
    "        return\n",
    "    df = pd.read_csv(csv_path).sort_values(\"pct\")\n",
    "    fig_path = FIG_DIR / \"data_efficiency_curve_sf.png\"\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df[\"pct\"], df[\"best_val_miou\"], marker=\"o\", label=\"SegFormer\")\n",
    "    plt.xlabel(\"% training data\"); plt.ylabel(\"Best Val mIoU\")\n",
    "    plt.title(\"Data efficiency (SegFormer): mIoU vs % training data\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=200); plt.close()\n",
    "    print(\"Saved:\", fig_path)\n",
    "    mark_done(\"curve_sf_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_plot_curve_compare():\n",
    "    cnn_csv = TABLE_DIR / \"cnn_data_efficiency_summary.csv\"\n",
    "    sf_csv  = TABLE_DIR / \"sf_data_efficiency_summary.csv\"\n",
    "    if (not cnn_csv.exists()) or (not sf_csv.exists()):\n",
    "        print(\"[WARN] Necesitas ambos CSV (CNN y SF) para la curva comparativa.\")\n",
    "        return\n",
    "    d1 = pd.read_csv(cnn_csv).sort_values(\"pct\")[[\"pct\",\"best_val_miou\"]].rename(columns={\"best_val_miou\":\"cnn_miou\"})\n",
    "    d2 = pd.read_csv(sf_csv).sort_values(\"pct\")[[\"pct\",\"best_val_miou\"]].rename(columns={\"best_val_miou\":\"sf_miou\"})\n",
    "    df = d1.merge(d2, on=\"pct\")\n",
    "    fig_path = FIG_DIR / \"data_efficiency_curve_compare.png\"\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(df[\"pct\"], df[\"cnn_miou\"], marker=\"o\", label=\"CNN (DeepLabV3)\")\n",
    "    plt.plot(df[\"pct\"], df[\"sf_miou\"],  marker=\"o\", label=\"SegFormer\")\n",
    "    plt.xlabel(\"% training data\"); plt.ylabel(\"Best Val mIoU\")\n",
    "    plt.title(\"Data efficiency: CNN vs SegFormer (Cityscapes)\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=200); plt.close()\n",
    "    print(\"Saved:\", fig_path)\n",
    "    mark_done(\"curve_compare_done\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eficiencia (latencia + VRAM + params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params_m(model):\n",
    "    return sum(p.numel() for p in model.parameters()) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def measure_inference(model, loader, forward_fn, n_warmup=EFF_WARMUP_STEPS, n_steps=EFF_MEASURE_STEPS):\n",
    "    model.eval()\n",
    "    if device.type == \"cuda\":\n",
    "        for gi in range(torch.cuda.device_count()):\n",
    "            torch.cuda.reset_peak_memory_stats(gi)\n",
    "\n",
    "    it = iter(loader)\n",
    "    for _ in range(n_warmup):\n",
    "        try:\n",
    "            batch = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            batch = next(it)\n",
    "        _ = forward_fn(batch)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.time()\n",
    "    n_imgs = 0\n",
    "    for _ in range(n_steps):\n",
    "        try:\n",
    "            batch = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            batch = next(it)\n",
    "        _ = forward_fn(batch)\n",
    "        n_imgs += batch[0].shape[0]\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    ms_per_img = (dt / max(n_imgs, 1)) * 1000\n",
    "\n",
    "    vram = {}\n",
    "    if device.type == \"cuda\":\n",
    "        for gi in range(torch.cuda.device_count()):\n",
    "            vram[gi] = torch.cuda.max_memory_allocated(gi) / (1024**3)\n",
    "    return ms_per_img, vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_cnn_100():\n",
    "    ckpt = CKPT_DIR / \"best_cnn_pct100.pth\"\n",
    "    if not ckpt.exists():\n",
    "        raise FileNotFoundError(\"No existe best_cnn_pct100.pth. Entrena CNN (pct100) primero.\")\n",
    "    model = build_cnn_model()\n",
    "    load_pth_weights(model, ckpt)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_sf_100():\n",
    "    ckpt = CKPT_DIR / \"best_sf_pct100.pth\"\n",
    "    if not ckpt.exists():\n",
    "        raise FileNotFoundError(\"No existe best_sf_pct100.pth. Entrena SF (pct100) primero.\")\n",
    "    model = build_segformer_model()\n",
    "    load_pth_weights(model, ckpt, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_efficiency_cnn():\n",
    "    cnn = load_best_cnn_100()\n",
    "    val_loader = make_loader_cnn(val_pairs, train=False)\n",
    "\n",
    "    def fwd(batch):\n",
    "        imgs, _ = batch\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        out = cnn(imgs)[\"out\"]\n",
    "        _ = out.argmax(1)\n",
    "        return out\n",
    "\n",
    "    ms, vram = measure_inference(cnn, val_loader, fwd)\n",
    "    out_path = TABLE_DIR / \"efficiency_cnn.csv\"\n",
    "    pd.DataFrame([{\n",
    "        \"model\":\"CNN (DeepLabV3)\",\n",
    "        \"latency_ms_per_img\": ms,\n",
    "        \"params_m\": count_params_m(cnn),\n",
    "        \"vram_gb_gpu0\": vram.get(0, None),\n",
    "        \"vram_gb_gpu1\": vram.get(1, None),\n",
    "        \"vram_gb_gpu2\": vram.get(2, None),\n",
    "    }]).to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    mark_done(\"eff_cnn_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_efficiency_sf():\n",
    "    sf = load_best_sf_100()\n",
    "    val_loader = make_loader_sf(val_pairs, train=False)\n",
    "\n",
    "    def fwd(batch):\n",
    "        px, _ = batch\n",
    "        px = px.to(device, non_blocking=True)\n",
    "        out = sf(pixel_values=px)[\"logits\"]\n",
    "        _ = out.argmax(1)\n",
    "        return out\n",
    "\n",
    "    ms, vram = measure_inference(sf, val_loader, fwd)\n",
    "    out_path = TABLE_DIR / \"efficiency_sf.csv\"\n",
    "    pd.DataFrame([{\n",
    "        \"model\":\"SegFormer\",\n",
    "        \"latency_ms_per_img\": ms,\n",
    "        \"params_m\": count_params_m(sf),\n",
    "        \"vram_gb_gpu0\": vram.get(0, None),\n",
    "        \"vram_gb_gpu1\": vram.get(1, None),\n",
    "        \"vram_gb_gpu2\": vram.get(2, None),\n",
    "    }]).to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    mark_done(\"eff_sf_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_efficiency_compare():\n",
    "    cnn_csv = TABLE_DIR / \"efficiency_cnn.csv\"\n",
    "    sf_csv  = TABLE_DIR / \"efficiency_sf.csv\"\n",
    "    if (not cnn_csv.exists()) or (not sf_csv.exists()):\n",
    "        print(\"[WARN] Necesitas efficiency_cnn.csv y efficiency_sf.csv para comparar.\")\n",
    "        return\n",
    "    df = pd.concat([pd.read_csv(cnn_csv), pd.read_csv(sf_csv)], ignore_index=True)\n",
    "    out_path = TABLE_DIR / \"efficiency_compare.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    mark_done(\"eff_compare_done\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZACIÓN: GT vs Pred + clean vs corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITYSCAPES_COLORS = np.array([\n",
    "    [128,  64, 128],[244,  35, 232],[ 70,  70,  70],[102, 102, 156],[190, 153, 153],\n",
    "    [153, 153, 153],[250, 170,  30],[220, 220,   0],[107, 142,  35],[152, 251, 152],\n",
    "    [ 70, 130, 180],[220,  20,  60],[255,   0,   0],[  0,   0, 142],[  0,   0,  70],\n",
    "    [  0,  60, 100],[  0,  80, 100],[  0,   0, 230],[119,  11,  32],\n",
    "], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_trainid(mask_trainid):\n",
    "    h, w = mask_trainid.shape\n",
    "    out = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    valid = (mask_trainid != IGNORE_INDEX)\n",
    "    out[valid] = CITYSCAPES_COLORS[mask_trainid[valid]]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def predict_cnn_single(img_pil, cnn_model):\n",
    "    cnn_model.eval()\n",
    "    img_r = img_pil.resize((RESIZE[1], RESIZE[0]), resample=Image.BILINEAR)\n",
    "    img_np = np.array(img_r, dtype=np.float32) / 255.0\n",
    "\n",
    "    x = torch.from_numpy(img_np).permute(2,0,1).unsqueeze(0).to(device)\n",
    "    x = (x - IMAGENET_MEAN.to(device)) / IMAGENET_STD.to(device)\n",
    "\n",
    "    out = cnn_model(x)\n",
    "    logits = out[\"out\"] if isinstance(out, dict) else out\n",
    "    pred = logits.argmax(1).squeeze(0).cpu().numpy().astype(np.int64)\n",
    "    return pred, img_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_sf_single(img_pil, sf_model):\n",
    "    img_r = img_pil.resize((RESIZE[1], RESIZE[0]), resample=Image.BILINEAR)\n",
    "    enc = processor(images=img_r, return_tensors=\"pt\")\n",
    "    px = enc[\"pixel_values\"].to(device)\n",
    "    logits = sf_model(pixel_values=px)[\"logits\"]\n",
    "    pred = logits.argmax(1).squeeze(0).detach().cpu().numpy().astype(np.int64)\n",
    "    return pred, img_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_gt_pred_cnn(pair_list, k, cnn_model, out_path):\n",
    "    img_path, mask_path = pair_list[k]\n",
    "    img  = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path)\n",
    "    img_r  = img.resize((RESIZE[1], RESIZE[0]), resample=Image.BILINEAR)\n",
    "    mask_r = mask.resize((RESIZE[1], RESIZE[0]), resample=Image.NEAREST)\n",
    "    gt = ID_TO_TRAINID[np.array(mask_r, dtype=np.uint8)].astype(np.int64)\n",
    "    pred, _ = predict_cnn_single(img, cnn_model)\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,3,1); plt.title(\"Image\"); plt.imshow(img_r); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,2); plt.title(\"GT\"); plt.imshow(colorize_trainid(gt)); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,3); plt.title(\"CNN Pred\"); plt.imshow(colorize_trainid(pred)); plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    out_path = Path(out_path)\n",
    "    plt.savefig(out_path, dpi=200); plt.close()\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_gt_pred_sf(pair_list, k, sf_model, out_path):\n",
    "    img_path, mask_path = pair_list[k]\n",
    "    img  = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path)\n",
    "    img_r  = img.resize((RESIZE[1], RESIZE[0]), resample=Image.BILINEAR)\n",
    "    mask_r = mask.resize((RESIZE[1], RESIZE[0]), resample=Image.NEAREST)\n",
    "    gt = ID_TO_TRAINID[np.array(mask_r, dtype=np.uint8)].astype(np.int64)\n",
    "    pred, _ = predict_sf_single(img, sf_model)\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,3,1); plt.title(\"Image\"); plt.imshow(img_r); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,2); plt.title(\"GT\"); plt.imshow(colorize_trainid(gt)); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,3); plt.title(\"SegFormer Pred\"); plt.imshow(colorize_trainid(pred)); plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    out_path = Path(out_path)\n",
    "    plt.savefig(out_path, dpi=200); plt.close()\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_gt_pred_compare(pair_list, k, cnn_model, sf_model, out_path):\n",
    "    img_path, mask_path = pair_list[k]\n",
    "    img  = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path)\n",
    "    img_r  = img.resize((RESIZE[1], RESIZE[0]), resample=Image.BILINEAR)\n",
    "    mask_r = mask.resize((RESIZE[1], RESIZE[0]), resample=Image.NEAREST)\n",
    "    gt = ID_TO_TRAINID[np.array(mask_r, dtype=np.uint8)].astype(np.int64)\n",
    "    pred_cnn, _ = predict_cnn_single(img, cnn_model)\n",
    "    pred_sf,  _ = predict_sf_single(img, sf_model)\n",
    "\n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.subplot(1,4,1); plt.title(\"Image\"); plt.imshow(img_r); plt.axis(\"off\")\n",
    "    plt.subplot(1,4,2); plt.title(\"GT\"); plt.imshow(colorize_trainid(gt)); plt.axis(\"off\")\n",
    "    plt.subplot(1,4,3); plt.title(\"CNN Pred\"); plt.imshow(colorize_trainid(pred_cnn)); plt.axis(\"off\")\n",
    "    plt.subplot(1,4,4); plt.title(\"SegFormer Pred\"); plt.imshow(colorize_trainid(pred_sf)); plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    out_path = Path(out_path)\n",
    "    plt.savefig(out_path, dpi=200); plt.close()\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_visual_cnn():\n",
    "    cnn = load_best_cnn_100()\n",
    "    cnn.eval()\n",
    "\n",
    "    rng = np.random.RandomState(VIS_RANDOM_SEED)\n",
    "    idxs = rng.choice(len(val_pairs), size=min(N_VIS_SAMPLES, len(val_pairs)), replace=False)\n",
    "\n",
    "    for j, k in enumerate(idxs):\n",
    "        outp = FIG_DIR / f\"val_cnn_gt_vs_pred_{j}_k{k}.png\"\n",
    "        save_gt_pred_cnn(val_pairs, int(k), cnn, outp)\n",
    "\n",
    "    mark_done(\"vis_cnn_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_visual_sf():\n",
    "    sf = load_best_sf_100()\n",
    "    rng = np.random.RandomState(VIS_RANDOM_SEED)\n",
    "    idxs = rng.choice(len(val_pairs), size=min(N_VIS_SAMPLES, len(val_pairs)), replace=False)\n",
    "    for j, k in enumerate(idxs):\n",
    "        outp = FIG_DIR / f\"val_sf_gt_vs_pred_{j}_k{k}.png\"\n",
    "        save_gt_pred_sf(val_pairs, int(k), sf, outp)\n",
    "    mark_done(\"vis_sf_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_visual_compare():\n",
    "    cnn_ckpt = CKPT_DIR / \"best_cnn_pct100.pth\"\n",
    "    sf_ckpt  = CKPT_DIR / \"best_sf_pct100.pth\"\n",
    "    if (not cnn_ckpt.exists()) or (not sf_ckpt.exists()):\n",
    "        print(\"[WARN] Necesitas ambos checkpoints pct100 para visual comparativa.\")\n",
    "        return\n",
    "    cnn = load_best_cnn_100()\n",
    "    sf  = load_best_sf_100()\n",
    "    rng = np.random.RandomState(VIS_RANDOM_SEED)\n",
    "    idxs = rng.choice(len(val_pairs), size=min(N_VIS_SAMPLES, len(val_pairs)), replace=False)\n",
    "    for j, k in enumerate(idxs):\n",
    "        outp = FIG_DIR / f\"val_compare_gt_vs_pred_{j}_k{k}.png\"\n",
    "        save_gt_pred_compare(val_pairs, int(k), cnn, sf, outp)\n",
    "    mark_done(\"vis_compare_done\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_corruptions_cnn():\n",
    "    cnn = load_best_cnn_100()\n",
    "\n",
    "    clean_loader = make_loader_cnn(val_pairs, train=False)\n",
    "    clean_miou, clean_acc, _ = evaluate_cnn(cnn, clean_loader)\n",
    "\n",
    "    rows = []\n",
    "    for corr, sev in CORRUPTIONS:\n",
    "        loader = make_loader_cnn(val_pairs, train=False, corruption=corr, severity=sev)\n",
    "        miou, acc, _ = evaluate_cnn(cnn, loader)\n",
    "        rows.append({\n",
    "            \"corruption\": corr, \"severity\": sev,\n",
    "            \"miou\": miou, \"acc\": acc,\n",
    "            \"delta_miou\": clean_miou - miou,\n",
    "            \"delta_acc\": clean_acc - acc\n",
    "        })\n",
    "        print(f\"[CNN {corr} s{sev}] mIoU={miou:.4f} Δ={clean_miou-miou:.4f}\")\n",
    "\n",
    "    out_path = TABLE_DIR / \"robustness_cnn.csv\"\n",
    "    pd.DataFrame(rows).to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    mark_done(\"corr_cnn_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_corruptions_sf():\n",
    "    sf = load_best_sf_100()\n",
    "\n",
    "    clean_loader = make_loader_sf(val_pairs, train=False)\n",
    "    clean_miou, clean_acc, _ = evaluate_sf(sf, clean_loader)\n",
    "\n",
    "    rows = []\n",
    "    for corr, sev in CORRUPTIONS:\n",
    "        loader = make_loader_sf(val_pairs, train=False, corruption=corr, severity=sev)\n",
    "        miou, acc, _ = evaluate_sf(sf, loader)\n",
    "        rows.append({\n",
    "            \"corruption\": corr, \"severity\": sev,\n",
    "            \"miou\": miou, \"acc\": acc,\n",
    "            \"delta_miou\": clean_miou - miou,\n",
    "            \"delta_acc\": clean_acc - acc\n",
    "        })\n",
    "        print(f\"[SF {corr} s{sev}] mIoU={miou:.4f} Δ={clean_miou-miou:.4f}\")\n",
    "\n",
    "    out_path = TABLE_DIR / \"robustness_sf.csv\"\n",
    "    pd.DataFrame(rows).to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    mark_done(\"corr_sf_done\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_corruptions_compare():\n",
    "    cnn_csv = TABLE_DIR / \"robustness_cnn.csv\"\n",
    "    sf_csv  = TABLE_DIR / \"robustness_sf.csv\"\n",
    "    if (not cnn_csv.exists()) or (not sf_csv.exists()):\n",
    "        print(\"[WARN] Necesitas robustness_cnn.csv y robustness_sf.csv para comparar.\")\n",
    "        return\n",
    "    d1 = pd.read_csv(cnn_csv).rename(columns={\"miou\":\"cnn_miou\",\"acc\":\"cnn_acc\",\"delta_miou\":\"cnn_delta_miou\",\"delta_acc\":\"cnn_delta_acc\"})\n",
    "    d2 = pd.read_csv(sf_csv).rename(columns={\"miou\":\"sf_miou\",\"acc\":\"sf_acc\",\"delta_miou\":\"sf_delta_miou\",\"delta_acc\":\"sf_delta_acc\"})\n",
    "    df = d1.merge(d2, on=[\"corruption\",\"severity\"])\n",
    "    out_path = TABLE_DIR / \"robustness_compare.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    mark_done(\"corr_compare_done\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(stages=None):\n",
    "    \"\"\"\n",
    "    stages (examples):\n",
    "      - [\"train_cnn\"]         -> only CNN training (all pcts)\n",
    "      - [\"train_sf\"]          -> only SegFormer training (all pcts)\n",
    "      - [\"curve_cnn\"]         -> only CNN curve\n",
    "      - [\"curve_sf\"]          -> only SF curve\n",
    "      - [\"curve_compare\"]     -> comparative curve (needs both)\n",
    "      - [\"eff_cnn\"]           -> CNN efficiency\n",
    "      - [\"eff_sf\"]            -> SF efficiency\n",
    "      - [\"eff_compare\"]       -> compare efficiency (needs both)\n",
    "      - [\"vis_cnn\"]           -> CNN GT vs Pred\n",
    "      - [\"vis_sf\"]            -> SF GT vs Pred\n",
    "      - [\"vis_compare\"]       -> combined (needs both)\n",
    "      - [\"corr_cnn\"]          -> corruptions CNN\n",
    "      - [\"corr_sf\"]           -> corruptions SF\n",
    "      - [\"corr_compare\"]      -> compare corruptions (needs both)\n",
    "      - [\"all_cnn\"]           -> CNN-only full pipeline\n",
    "      - [\"all_sf\"]            -> SF-only full pipeline\n",
    "      - [\"all_compare\"]       -> only comparisons (requires both done)\n",
    "      - [\"all\"]               -> everything (cnn then sf then comparisons)\n",
    "    \"\"\"\n",
    "    if stages is None:\n",
    "        stages = [\"all\"]\n",
    "\n",
    "    if \"all\" in stages:\n",
    "        stages = [\n",
    "            \"train_cnn\",\"curve_cnn\",\"eff_cnn\",\"vis_cnn\",\"corr_cnn\",\n",
    "            \"train_sf\",\"curve_sf\",\"eff_sf\",\"vis_sf\",\"corr_sf\",\n",
    "            \"curve_compare\",\"eff_compare\",\"vis_compare\",\"corr_compare\"\n",
    "        ]\n",
    "\n",
    "    if \"all_cnn\" in stages:\n",
    "        stages = [\"train_cnn\",\"curve_cnn\",\"eff_cnn\",\"vis_cnn\",\"corr_cnn\"]\n",
    "\n",
    "    if \"all_sf\" in stages:\n",
    "        stages = [\"train_sf\",\"curve_sf\",\"eff_sf\",\"vis_sf\",\"corr_sf\"]\n",
    "\n",
    "    if \"all_compare\" in stages:\n",
    "        stages = [\"curve_compare\",\"eff_compare\",\"vis_compare\",\"corr_compare\"]\n",
    "\n",
    "    # --- CNN pipeline\n",
    "    if \"train_cnn\" in stages:\n",
    "        stage_train_cnn_all_pcts()\n",
    "    if \"curve_cnn\" in stages:\n",
    "        stage_plot_curve_cnn()\n",
    "    if \"eff_cnn\" in stages:\n",
    "        stage_efficiency_cnn()\n",
    "    if \"vis_cnn\" in stages:\n",
    "        stage_visual_cnn()\n",
    "    if \"corr_cnn\" in stages:\n",
    "        stage_corruptions_cnn()\n",
    "\n",
    "    # --- SegFormer pipeline\n",
    "    if \"train_sf\" in stages:\n",
    "        stage_train_sf_all_pcts()\n",
    "    if \"curve_sf\" in stages:\n",
    "        stage_plot_curve_sf()\n",
    "    if \"eff_sf\" in stages:\n",
    "        stage_efficiency_sf()\n",
    "    if \"vis_sf\" in stages:\n",
    "        stage_visual_sf()\n",
    "    if \"corr_sf\" in stages:\n",
    "        stage_corruptions_sf()\n",
    "\n",
    "    # --- Comparative\n",
    "    if \"curve_compare\" in stages:\n",
    "        stage_plot_curve_compare()\n",
    "    if \"eff_compare\" in stages:\n",
    "        stage_efficiency_compare()\n",
    "    if \"vis_compare\" in stages:\n",
    "        stage_visual_compare()\n",
    "    if \"corr_compare\" in stages:\n",
    "        stage_corruptions_compare()\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"DONE. Outputs in:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de mapeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\ALLAN TURING\\\\.cache\\\\kagglehub\\\\datasets\\\\electraawais\\\\cityscape-dataset\\\\versions\\\\2\\\\Cityscape Dataset\\\\leftImg8bit\\\\test\\\\berlin\\\\berlin_000000_000019_leftImg8bit.png', 'C:\\\\Users\\\\ALLAN TURING\\\\.cache\\\\kagglehub\\\\datasets\\\\electraawais\\\\cityscape-dataset\\\\versions\\\\2\\\\Fine Annotations\\\\gtFine\\\\test\\\\berlin\\\\berlin_000000_000019_gtFine_labelIds.png')\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(test_pairs[0])\n",
    "print(Path(test_pairs[0][0]).exists(), Path(test_pairs[0][1]).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw unique (first 30): [0 1 3]\n",
      "trainId unique: [255]\n",
      "ignore ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "img_path, mask_path = test_pairs[0]\n",
    "mask = Image.open(mask_path)\n",
    "mask_r = mask.resize((RESIZE[1], RESIZE[0]), resample=Image.NEAREST)\n",
    "gt_raw = np.array(mask_r, dtype=np.uint8)\n",
    "gt = ID_TO_TRAINID[gt_raw].astype(np.int64)\n",
    "\n",
    "print(\"raw unique (first 30):\", np.unique(gt_raw)[:30])\n",
    "print(\"trainId unique:\", np.unique(gt))\n",
    "print(\"ignore ratio:\", (gt == 255).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 -> 0\n",
      "8 -> 1\n",
      "11 -> 2\n",
      "24 -> 11\n",
      "26 -> 13\n",
      "33 -> 18\n"
     ]
    }
   ],
   "source": [
    "for lid in [7,8,11,24,26,33]:\n",
    "    print(lid, \"->\", int(ID_TO_TRAINID[lid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask file: C:\\Users\\ALLAN TURING\\.cache\\kagglehub\\datasets\\electraawais\\cityscape-dataset\\versions\\2\\Fine Annotations\\gtFine\\val\\frankfurt\\frankfurt_000000_000294_gtFine_labelIds.png\n",
      "raw unique (first 30): [ 1  2  3  4  7  8 11 13 17 20 21 23 24 26]\n",
      "trainId unique: [  0   1   2   4   5   7   8  10  11  13 255]\n",
      "ignore ratio: 0.11572837829589844\n"
     ]
    }
   ],
   "source": [
    "pairs = val_pairs  # usa val primero (recomendado)\n",
    "img_path, mask_path = pairs[0]\n",
    "\n",
    "mask = Image.open(mask_path)\n",
    "mask_r = mask.resize((RESIZE[1], RESIZE[0]), resample=Image.NEAREST)\n",
    "\n",
    "gt_raw = np.array(mask_r, dtype=np.uint8)\n",
    "gt = ID_TO_TRAINID[gt_raw].astype(np.int64)\n",
    "\n",
    "print(\"mask file:\", mask_path)\n",
    "print(\"raw unique (first 30):\", np.unique(gt_raw)[:30])\n",
    "print(\"trainId unique:\", np.unique(gt))\n",
    "print(\"ignore ratio:\", float((gt == 255).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs: torch.Size([4, 3, 512, 1024])\n",
      "logits: torch.Size([4, 19, 512, 1024])\n",
      "NUM_CLASSES: 19\n"
     ]
    }
   ],
   "source": [
    "cnn = load_best_cnn_100()\n",
    "cnn.eval()\n",
    "\n",
    "loader = make_loader_cnn(val_pairs, train=False)\n",
    "imgs, masks = next(iter(loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = cnn(imgs.to(device))\n",
    "logits = out[\"out\"] if isinstance(out, dict) else out\n",
    "\n",
    "print(\"imgs:\", imgs.shape)\n",
    "print(\"logits:\", logits.shape)  # debe ser [B, 19, H, W]\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN VAL: 0.7217209339141846 0.9528476710590452\n"
     ]
    }
   ],
   "source": [
    "cnn = load_best_cnn_100()\n",
    "val_loader = make_loader_cnn(val_pairs, train=False)\n",
    "miou, acc, _ = evaluate_cnn(cnn, val_loader)\n",
    "print(\"CLEAN VAL:\", miou, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN VAL: 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "cnn = load_best_cnn_100()\n",
    "val_loader = make_loader_cnn(test_pairs, train=False)\n",
    "miou, acc, _ = evaluate_cnn(cnn, val_loader)\n",
    "print(\"CLEAN VAL:\", miou, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Robustness experiments are reported on the Cityscapes validation split, since the official test set is intended for server-side evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN DATA EFFICIENCY: pct=10 | train_samples=297\n",
      "[SKIP] CNN pct=10 ya entrenado. CKPT: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\checkpoints\\best_cnn_pct10.pth\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\cnn_data_efficiency_summary.csv\n",
      "CNN DATA EFFICIENCY: pct=25 | train_samples=743\n",
      "[SKIP] CNN pct=25 ya entrenado. CKPT: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\checkpoints\\best_cnn_pct25.pth\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\cnn_data_efficiency_summary.csv\n",
      "CNN DATA EFFICIENCY: pct=50 | train_samples=1487\n",
      "[SKIP] CNN pct=50 ya entrenado. CKPT: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\checkpoints\\best_cnn_pct50.pth\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\cnn_data_efficiency_summary.csv\n",
      "CNN DATA EFFICIENCY: pct=100 | train_samples=2975\n",
      "[SKIP] CNN pct=100 ya entrenado. CKPT: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\checkpoints\\best_cnn_pct100.pth\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\cnn_data_efficiency_summary.csv\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\data_efficiency_curve_cnn.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\efficiency_cnn.csv\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_cnn_gt_vs_pred_0_k195.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_cnn_gt_vs_pred_1_k417.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_cnn_gt_vs_pred_2_k478.png\n",
      "[CNN gaussian_blur s3] mIoU=0.5821 Δ=0.1396\n",
      "[CNN motion_blur s3] mIoU=0.6111 Δ=0.1106\n",
      "[CNN gaussian_noise s3] mIoU=0.0476 Δ=0.6741\n",
      "[CNN brightness s3] mIoU=0.7135 Δ=0.0082\n",
      "[CNN contrast s3] mIoU=0.7139 Δ=0.0078\n",
      "[CNN jpeg s3] mIoU=0.2830 Δ=0.4388\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\robustness_cnn.csv\n",
      "SF DATA EFFICIENCY: pct=10 | train_samples=297\n",
      "[DEBUG] SF loader smoke test...\n",
      "[DEBUG] SF batch OK: torch.Size([2, 3, 512, 512]) torch.Size([2, 512, 1024])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd074d19e6bf4b8cbaaf2782a8294f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 1/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 1/30 | loss=0.1381 | val mIoU=0.6311 | acc=0.9371 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40815449f539487a859598fb119dae05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 2/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 2/30 | loss=0.1251 | val mIoU=0.6369 | acc=0.9370 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dd0ea3febf4ab3aead1f945526cbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 3/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 3/30 | loss=0.1291 | val mIoU=0.6103 | acc=0.9320 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125ff7d7acec4366ba75903769121906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 4/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 4/30 | loss=0.1231 | val mIoU=0.6416 | acc=0.9388 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efea4b7c427e420f85c8508dcda353c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 5/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 5/30 | loss=0.1127 | val mIoU=0.6404 | acc=0.9384 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0116c44e34204449934c376cd3dec946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 6/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 6/30 | loss=0.1134 | val mIoU=0.6335 | acc=0.9377 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea96de6aec254b62b911bf77899f0d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 7/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 7/30 | loss=0.1111 | val mIoU=0.6386 | acc=0.9381 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecc45c014bb44a49542be5d46f9783c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 8/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 8/30 | loss=0.1099 | val mIoU=0.6276 | acc=0.9376 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586d53c8c2824b5e9eff2d643d6177a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 9/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 9/30 | loss=0.1059 | val mIoU=0.6426 | acc=0.9395 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eaaa99cec0640d8a0cb7b3a510d71d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 10/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 10/30 | loss=0.1033 | val mIoU=0.6438 | acc=0.9393 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b51b1b933a94b7eb821e5f4ead55fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 11/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 11/30 | loss=0.1003 | val mIoU=0.6413 | acc=0.9393 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896f803f52e2431b9d6ef81c6fa9de8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 12/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 12/30 | loss=0.1024 | val mIoU=0.6476 | acc=0.9393 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfacc7821614987b4fda16d580363ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 13/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 13/30 | loss=0.1012 | val mIoU=0.6398 | acc=0.9394 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e63e0eef1a04e5d9195a11bf2611a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 14/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 14/30 | loss=0.0966 | val mIoU=0.6429 | acc=0.9392 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3642c1d36946d6990fbba8965fc456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 15/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 15/30 | loss=0.0956 | val mIoU=0.6346 | acc=0.9394 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7ad446650445afa368ebbf51fe1d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 16/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 16/30 | loss=0.0960 | val mIoU=0.6409 | acc=0.9395 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f648ef3684404c468e782f9377a24f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 17/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 17/30 | loss=0.0950 | val mIoU=0.6347 | acc=0.9390 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b85ef0637dc443f824949e02469821e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 18/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 18/30 | loss=0.0926 | val mIoU=0.6346 | acc=0.9393 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855448dda36d424db4deabd6557c6b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 19/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 19/30 | loss=0.0931 | val mIoU=0.6312 | acc=0.9390 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63986beb7b14c198cc3ceaaecef0841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 20/30:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=10] Epoch 20/30 | loss=0.0922 | val mIoU=0.6373 | acc=0.9395 \n",
      "[SF pct=10] Early stopping.\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\sf_data_efficiency_summary.csv\n",
      "SF DATA EFFICIENCY: pct=25 | train_samples=743\n",
      "[DEBUG] SF loader smoke test...\n",
      "[DEBUG] SF batch OK: torch.Size([2, 3, 512, 512]) torch.Size([2, 512, 1024])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a1723a9f1b4475af206d6403ca671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 1/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 1/30 | loss=0.1378 | val mIoU=0.6402 | acc=0.9388 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dfac80f09f412abf44090d7d92b560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 2/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 2/30 | loss=0.1253 | val mIoU=0.6445 | acc=0.9385 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5986f87c74f94b39947808cd3692bccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 3/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 3/30 | loss=0.1316 | val mIoU=0.6376 | acc=0.9387 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f140f341ec4d6fa21f3c805cb872bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 4/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 4/30 | loss=0.1347 | val mIoU=0.6401 | acc=0.9365 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee852d32ad7246988b49782a45fe3ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 5/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 5/30 | loss=0.1187 | val mIoU=0.6313 | acc=0.9374 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f1a300bf1e49a39c6e9b36b98befb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 6/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 6/30 | loss=0.1195 | val mIoU=0.6475 | acc=0.9391 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d6d738f97c4067989d998969baa0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 7/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 7/30 | loss=0.1157 | val mIoU=0.6512 | acc=0.9398 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b38b30c22c24c7ebaebc0071eadd1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 8/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 8/30 | loss=0.1097 | val mIoU=0.6562 | acc=0.9405 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6f6e378e7242ba83399186f0deaf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 9/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 9/30 | loss=0.1101 | val mIoU=0.6362 | acc=0.9352 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed8e4466dd14bd2acaeed80087681be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 10/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 10/30 | loss=0.1189 | val mIoU=0.6482 | acc=0.9402 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7b1c1f7a0148e79300c04159496826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 11/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 11/30 | loss=0.1079 | val mIoU=0.6528 | acc=0.9405 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1facb355eaea4349a3d022e38b6f74d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 12/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 12/30 | loss=0.1042 | val mIoU=0.6547 | acc=0.9404 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff3cd4f569f48e1bcff0ad1cc5c4d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 13/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 13/30 | loss=0.1059 | val mIoU=0.6452 | acc=0.9396 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78aff6b56fc4fa18d9d058b7897f26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 14/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 14/30 | loss=0.1031 | val mIoU=0.6522 | acc=0.9397 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0368e41c070e462b806beb098d020afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 15/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 15/30 | loss=0.1017 | val mIoU=0.6442 | acc=0.9396 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee07db7d0eb243bf9039675ef2b5ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 16/30:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=25] Epoch 16/30 | loss=0.1001 | val mIoU=0.6446 | acc=0.9403 \n",
      "[SF pct=25] Early stopping.\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\sf_data_efficiency_summary.csv\n",
      "SF DATA EFFICIENCY: pct=50 | train_samples=1487\n",
      "[DEBUG] SF loader smoke test...\n",
      "[DEBUG] SF batch OK: torch.Size([2, 3, 512, 512]) torch.Size([2, 512, 1024])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32520732591549ed9a8068b1f87faabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 1/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 1/30 | loss=0.1388 | val mIoU=0.6416 | acc=0.9389 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548f3c402135480bae1c409659c5811d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 2/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 2/30 | loss=0.1289 | val mIoU=0.6420 | acc=0.9379 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2d50f458304f588d53e9ea10bdca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 3/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 3/30 | loss=0.1278 | val mIoU=0.6509 | acc=0.9395 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dd4ee9a95946e487f0466e6ed803c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 4/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 4/30 | loss=0.1315 | val mIoU=0.6323 | acc=0.9384 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb0d75706fd4a8c9a975aff18e7905b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 5/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 5/30 | loss=0.1242 | val mIoU=0.6543 | acc=0.9397 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6da968669084e38aebff628ec23d371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 6/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 6/30 | loss=0.1191 | val mIoU=0.6638 | acc=0.9408 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b02f7ab8954418bd494ce642be7b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 7/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 7/30 | loss=0.1202 | val mIoU=0.6590 | acc=0.9385 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604c23f85166463f92fc9e13a309ae56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 8/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 8/30 | loss=0.1216 | val mIoU=0.6488 | acc=0.9406 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef87a1b83c748f98f25136ef9e4d23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 9/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 9/30 | loss=0.1158 | val mIoU=0.6539 | acc=0.9400 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a87dfe92924e459a3e2ad0d1f7fc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 10/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 10/30 | loss=0.1131 | val mIoU=0.6638 | acc=0.9415 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190a96e8a50641cbb3689cff2fd2d8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 11/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 11/30 | loss=0.1126 | val mIoU=0.6456 | acc=0.9402 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43680c7c905448f86dcd3c6a57e51a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 12/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 12/30 | loss=0.1095 | val mIoU=0.6622 | acc=0.9410 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccfa035c9b4486cbe64a480b4126ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 13/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 13/30 | loss=0.1080 | val mIoU=0.6609 | acc=0.9410 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e08223c520499ebb38329db7635288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 14/30:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=50] Epoch 14/30 | loss=0.1061 | val mIoU=0.6587 | acc=0.9412 \n",
      "[SF pct=50] Early stopping.\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\sf_data_efficiency_summary.csv\n",
      "SF DATA EFFICIENCY: pct=100 | train_samples=2975\n",
      "[DEBUG] SF loader smoke test...\n",
      "[DEBUG] SF batch OK: torch.Size([2, 3, 512, 512]) torch.Size([2, 512, 1024])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea76e5c84834bae90af0ca6c09b89dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 1/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 1/30 | loss=0.1347 | val mIoU=0.6495 | acc=0.9396 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef186b8e552e4477b58690ea39eb61d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 2/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 2/30 | loss=0.1258 | val mIoU=0.6557 | acc=0.9408 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fce5d447d143219c84d50f702bf3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 3/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 3/30 | loss=0.1288 | val mIoU=0.6377 | acc=0.9354 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c73ded98a9495384e4210ab4564380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 4/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 4/30 | loss=0.1309 | val mIoU=0.6626 | acc=0.9401 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a180862101804d75b321f41ffd94a179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 5/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 5/30 | loss=0.1248 | val mIoU=0.6577 | acc=0.9410 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a0ce513cdd48e38eeb6eb33e22cdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 6/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 6/30 | loss=0.1219 | val mIoU=0.6627 | acc=0.9418 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eedff672aeb4839b774471172d9434c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 7/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 7/30 | loss=0.1222 | val mIoU=0.6551 | acc=0.9401 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f31546fae77445b86ecf8b42592e849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 8/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 8/30 | loss=0.1187 | val mIoU=0.6623 | acc=0.9402 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fbbc6ad210420d9f4e38c52ed140c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 9/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 9/30 | loss=0.1193 | val mIoU=0.6631 | acc=0.9402 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0354ef392772461d8a4f0eb7ac4aafe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 10/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 10/30 | loss=0.1137 | val mIoU=0.6535 | acc=0.9400 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3225eb20df4e55850a674e7fc5ce60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 11/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 11/30 | loss=0.1128 | val mIoU=0.6721 | acc=0.9421 (best)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d91e3c383384c9bb5a81d5fce0e522e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 12/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 12/30 | loss=0.1104 | val mIoU=0.6696 | acc=0.9423 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b1e5d73d3a401aa946591a2f445b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 13/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 13/30 | loss=0.1094 | val mIoU=0.6711 | acc=0.9419 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876bdc3331cc41348893c41886b799cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 14/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 14/30 | loss=0.1074 | val mIoU=0.6613 | acc=0.9423 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06ab33ef5904212bda16d00d248a0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 15/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 15/30 | loss=0.1060 | val mIoU=0.6702 | acc=0.9423 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e39a4d256b418c8e6d0f004fd721d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 16/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 16/30 | loss=0.1052 | val mIoU=0.6695 | acc=0.9427 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6af1e1912741038479e50cda4af767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 17/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 17/30 | loss=0.1035 | val mIoU=0.6654 | acc=0.9421 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd26eef37a1f4140b884411478e2f3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 18/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 18/30 | loss=0.1027 | val mIoU=0.6640 | acc=0.9423 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6875a5c17e654be5858879ed0fa7eada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SF Epoch 19/30:   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SF pct=100] Epoch 19/30 | loss=0.1015 | val mIoU=0.6692 | acc=0.9429 \n",
      "[SF pct=100] Early stopping.\n",
      "Saved/Updated: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\sf_data_efficiency_summary.csv\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\data_efficiency_curve_sf.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\efficiency_sf.csv\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_sf_gt_vs_pred_0_k195.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_sf_gt_vs_pred_1_k417.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_sf_gt_vs_pred_2_k478.png\n",
      "[SF gaussian_blur s3] mIoU=0.6076 Δ=0.0645\n",
      "[SF motion_blur s3] mIoU=0.5976 Δ=0.0746\n",
      "[SF gaussian_noise s3] mIoU=0.4984 Δ=0.1737\n",
      "[SF brightness s3] mIoU=0.6641 Δ=0.0080\n",
      "[SF contrast s3] mIoU=0.6673 Δ=0.0048\n",
      "[SF jpeg s3] mIoU=0.5498 Δ=0.1223\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\robustness_sf.csv\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\data_efficiency_curve_compare.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\efficiency_compare.csv\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_compare_gt_vs_pred_0_k195.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_compare_gt_vs_pred_1_k417.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\figures\\val_compare_gt_vs_pred_2_k478.png\n",
      "Saved: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\\tables\\robustness_compare.csv\n",
      "DONE. Outputs in: C:\\Users\\ALLAN TURING\\Documents\\ProyectoTE_AyalaMacias\\outputs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Examples:\n",
    "    # run([\"train_cnn\"])  -> only baseline, resumable\n",
    "    # run([\"train_sf\"])   -> only segformer, resumable\n",
    "    # run([\"all_cnn\"])    -> full CNN pipeline only\n",
    "    # run([\"all_sf\"])     -> full SF pipeline only\n",
    "    # run([\"all_compare\"]) -> only comparative outputs (needs both)\n",
    "    run([\"all\"])         # -> all stages\n",
    "    # run([\"corr_cnn\", \"all_sf\", \"all_compare\"])2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8346584,
     "sourceId": 13189886,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
